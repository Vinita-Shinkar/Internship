{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEB SCRAPING ASSIGNMENT - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. Write a python program which searches all the product under a particular product vertical from www.amazon.in. The product verticals to be searched will be taken as input from user. For e.g. If user input is 'guitar'. Then search for guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\vinit\\anaconda3\\lib\\site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\vinit\\anaconda3\\lib\\site-packages (from selenium) (1.25.11)\n"
     ]
    }
   ],
   "source": [
    "# Let's first install the selenium library\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first connect with the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\vinit\\Downloads\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "url =\"https://www.amazon.in\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"1679c01dc6a09e3afd77e41dbb21a9e8\", element=\"4b2bfeaa-c364-424e-9304-f0d256bc198c\")>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding element for product verticals to be searched using xpath\n",
    "search_products = driver.find_element_by_xpath(\"//input[@aria-label='Search']\")\n",
    "search_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Television\n"
     ]
    }
   ],
   "source": [
    "# Write on search bar\n",
    "search_products.send_keys(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the above case i took the input from the user and the product was Television in my case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"1679c01dc6a09e3afd77e41dbb21a9e8\", element=\"81464bd4-1470-4c4a-92c3-94e2322dcdc8\")>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding element for search button using xpath\n",
    "search_btn = driver.find_element_by_xpath(\"//span[@class='nav-search-submit-text nav-sprite nav-progressive-attribute']\")\n",
    "search_btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click the search button\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a dataframe and csv. In case if any product vertical has less than 3 pages in search results then scrape all the products available under that product vertical. Details to be scrapped are: \"Brand Name\", \"Name of the Product\", \"Rating\", \"No. of Ratings\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\", \"Other Details\" and \"Product URL\". In case, if any of the details are missing for any of the product then replace it by \"-\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the element for different brands url using xpath\n",
    "brands_url = driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']//a\")\n",
    "\n",
    "# We will run a loop to iterate all the url links extracted above and extract the href inside them\n",
    "Brands_Url = []\n",
    "for i in range(1,5):\n",
    "    try:\n",
    "        for i in brands_url:\n",
    "            Brands_Url.append(i.get_attribute(\"href\"))\n",
    "    except:\n",
    "        Brands_Url.append(\"-\")\n",
    "              \n",
    "Brands = []\n",
    "Product_Name = []\n",
    "Ratings = []\n",
    "Ratings_Number = []\n",
    "Price = []\n",
    "Exchange = []\n",
    "Ex_Delivery = []\n",
    "Available = []\n",
    "Details = []\n",
    "\n",
    "# Let's traverse all URL one by one\n",
    "for i in Brands_Url:\n",
    "    driver.get(i)\n",
    "    driver.implicitly_wait(5)\n",
    "       \n",
    "    # Finding the elements for Brand names using id\n",
    "    try: \n",
    "        Brands.append(driver.find_element_by_id(\"productTitle\").text.split(' ')[0]) \n",
    "    except: \n",
    "        Brands.append('-')\n",
    "    driver.implicitly_wait(5)\n",
    "    \n",
    "    # Finding the elements for product names using id\n",
    "    try: \n",
    "        string=''\n",
    "        for i in driver.find_element_by_id(\"productTitle\").text.split(' ')[1:]:\n",
    "            string=string+' '+i\n",
    "        Product_Name.append(string)\n",
    "    except:\n",
    "        Product_Name.append('-')\n",
    "    driver.implicitly_wait(5)\n",
    "    \n",
    "    # Finding the elements for ratings using id\n",
    "    try: \n",
    "        Ratings.append(driver.find_element_by_id(\"acrPopover\").get_attribute('title')) \n",
    "    except: \n",
    "        Ratings.append('-')\n",
    "    driver.implicitly_wait(5)\n",
    "    \n",
    "    # Finding the elements for ratings number using id\n",
    "    try: \n",
    "        Ratings_Number.append(driver.find_element_by_id(\"acrCustomerReviewText\").text) \n",
    "    except: \n",
    "        Ratings_Number.append('-')\n",
    "    driver.implicitly_wait(5)\n",
    "    \n",
    "    # Finding the elements for prices using id\n",
    "    try: \n",
    "        try: \n",
    "            Price.append(driver.find_element_by_id(\"priceblock_saleprice\").text) \n",
    "        except:\n",
    "            try: \n",
    "                Price.append(driver.find_element_by_id(\"priceblock_dealprice\").text)\n",
    "            except: \n",
    "                Price.append(driver.find_element_by_id(\"priceblock_ourprice\").text)\n",
    "    except: \n",
    "        Price.append('-')\n",
    "    driver.implicitly_wait(5)\n",
    "    \n",
    "    # Finding the elements for return/exchange using xpath\n",
    "    try: \n",
    "        exchange = driver.find_element_by_xpath(\"//div[@class='a-row icon-farm-wrapper']\").find_elements_by_xpath(\".//div\")\n",
    "        found=False\n",
    "        for i in exchange:\n",
    "            if(i.get_attribute('data-name')=='RETURNS_POLICY'):\n",
    "                found=True\n",
    "                Exchange.append(i.find_element_by_xpath(\".//span[1]/div[2]/a\").text)\n",
    "        if(found==False):\n",
    "            Exchange.append('-')\n",
    "    except:\n",
    "        Exchange.append('-')\n",
    "    driver.implicitly_wait(5)\n",
    "    \n",
    "    # Finding the elements for expected delivery using id\n",
    "    try: \n",
    "        Ex_Delivery.append(driver.find_element_by_id(\"ddmDeliveryMessage\").find_element_by_xpath(\".//b\").text)\n",
    "    except: \n",
    "        Ex_Delivery.append('-')\n",
    "    driver.implicitly_wait(5)\n",
    "    \n",
    "    # Finding the elements for availability using id\n",
    "    try: \n",
    "        try: \n",
    "            Available.append(driver.find_element_by_id(\"availability\").find_element_by_xpath(\".//span\").text)\n",
    "        except:\n",
    "            Available.append(driver.find_element_by_id(\"deal_availability\").find_element_by_xpath(\".//div/span\").text)\n",
    "    except: \n",
    "        Available.append('-')\n",
    "    driver.implicitly_wait(5)\n",
    "    \n",
    "    # Finding the elements for details using id\n",
    "    try:\n",
    "        details = [i.text.replace('\\n','---') for i in driver.find_element_by_id(\"productDetails_techSpec_section_1\").find_elements_by_xpath(\".//tbody\")] \n",
    "        Details.append(details[0])\n",
    "    except: \n",
    "        Details.append('-')\n",
    "    driver.implicitly_wait(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "ques_2 = pd.DataFrame({})\n",
    "ques_2[\"BRAND NAME\"] = Brands\n",
    "ques_2[\"NAME OF THE PRODUCT\"] = Product_Name\n",
    "ques_2[\"RATINGS\"] = Ratings\n",
    "ques_2[\"NUMBER OF RATINGS\"] = Ratings_Number\n",
    "ques_2[\"PRICE\"] = Price\n",
    "ques_2[\"RETURN/EXCHANGE\"] = Exchange\n",
    "ques_2[\"EXPECTED DELIVERY\"] = Ex_Delivery\n",
    "ques_2[\"AVAILABLILITY\"] = Available\n",
    "ques_2[\"OTHER DETAILS\"] = Details\n",
    "ques_2[\"PRODUCT URL\"] = Brands_Url\n",
    "ques_2.index+=1\n",
    "\n",
    "# Details of each product in first 3 pages\n",
    "ques_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the dataframe in csv file\n",
    "ques_2.to_csv(\"product_details.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. Write a python program to access the search bar and search button on images.google.com and scrape 100 images each for keywords 'fruits', 'cars' and 'Machine Learning'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first connect with the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\vinit\\Downloads\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_3 = \"https://images.google.com/\"\n",
    "driver.get(url_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the element for search bar using xpath\n",
    "search = driver.find_element_by_xpath(\"//input[@class='gLFyf gsfi']\")\n",
    "\n",
    "# Writing in the search bar\n",
    "search.send_keys(\"Fruits\")\n",
    "\n",
    "# Finding the element for search button using xpath\n",
    "search_btn = driver.find_element_by_xpath(\"//span[@class='z1asCe MZy1Rb']\")\n",
    "\n",
    "# Clicking the search button\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fruit_Images = []\n",
    "for i in range(1,2):\n",
    "    try:\n",
    "        # Finding the element for Fruit Images using xpath\n",
    "        fruit_images = driver.find_elements_by_xpath(\"//div[@class='bRMDJf islir']//img\")\n",
    "        # We will run a loop to iterate all the Fruit Images tags extracted above and extract the text inside them\n",
    "        for i in fruit_images:\n",
    "            Fruit_Images.append(i.get_attribute('src'))\n",
    "    except:\n",
    "        Fruit_Images.append(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Fruit_Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clear the search bar using xpath\n",
    "search = driver.find_element_by_xpath(\"//input[@class='og3lId']\").clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the element for search bar using xpath\n",
    "search = driver.find_element_by_xpath(\"//input[@class='og3lId']\")\n",
    "\n",
    "# Writing in the search bar\n",
    "search.send_keys(\"Cars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the element for search button using xpath\n",
    "search_btn = driver.find_element_by_xpath(\"//span[@class='n6h3Rc']\")\n",
    "\n",
    "# Clicking the search button\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Car_Images = []\n",
    "for i in range(1,2):\n",
    "    try:\n",
    "        # Finding the element for Car Images using xpath\n",
    "        car_images = driver.find_elements_by_xpath(\"//div[@class='bRMDJf islir']//img\")\n",
    "        # We will run a loop to iterate all the Car Images tags extracted above and extract the text inside them\n",
    "        for i in car_images:\n",
    "            Car_Images.append(i.get_attribute('src'))\n",
    "    except:\n",
    "        Car_Images.append(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Car_Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clear the search bar using xpath\n",
    "search = driver.find_element_by_xpath(\"//input[@class='og3lId']\").clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the element for search bar using xpath\n",
    "search = driver.find_element_by_xpath(\"//input[@class='og3lId']\")\n",
    "\n",
    "# Writing in the search bar\n",
    "search.send_keys(\"Machine Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the element for search button using xpath\n",
    "search_btn = driver.find_element_by_xpath(\"//span[@class='n6h3Rc']\")\n",
    "\n",
    "# Clicking the search button\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_Images = []\n",
    "for i in range(1,2):\n",
    "    try:\n",
    "        # Finding the element for Machine Learning Images using xpath\n",
    "        ml_images = driver.find_elements_by_xpath(\"//div[@class='bRMDJf islir']//img\")\n",
    "        # We will run a loop to iterate all the Machine Learning Images tags extracted above and extract the text inside them\n",
    "        for i in ml_images:\n",
    "            ML_Images.append(i.get_attribute('src'))\n",
    "    except:\n",
    "        ML_Images.append(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ML_Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "ques_3 = pd.DataFrame({})\n",
    "ques_3['FRUIT IMAGES'] = Fruit_Images[:100]\n",
    "ques_3['CAR IMAGES'] = Car_Images[:100]\n",
    "ques_3['MACHINE LEARNING IMAGES'] = ML_Images[:100]\n",
    "ques_3.index+=1\n",
    "\n",
    "# First 100 images of Fruits, Cars and Machine Learning\n",
    "ques_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. Write a python program to search for a smartphone(e.g: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: \"Brand Name\", \"Smartphone name\", \"Colour\", \"RAM\", \"Storage(ROM)\", \"Primary Camera\", \"Secondary Camera\", \"Display Size\", \"Display Resolution\", \"Processor\", \"Processor Cores\", \"Battery Capacity\", \"Price\", \"Product URL\". Incase if any of the details is missing then replace it by \"-\". Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required Libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "# importing regex\n",
    "import re\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first connect with the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\vinit\\Downloads\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_4 = \"https://www.flipkart.com/\"\n",
    "driver.get(url_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the element for search bar using xpath\n",
    "search = driver.find_element_by_xpath(\"//input[@title='Search for products, brands and more']\")\n",
    "\n",
    "# Writing in the search bar\n",
    "search.send_keys(\"Pixel 4A\")\n",
    "\n",
    "# Finding the element for search button using xpath\n",
    "search_btn = driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "\n",
    "# Clicking the search button\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding element for smartphone name using xpath\n",
    "smart_name = driver.find_elements_by_xpath(\"//div[@class='_4rR01T']\")\n",
    "\n",
    "# We will run a loop to iterate all the smartphone name tags extracted above and extract the text inside them\n",
    "Smart_Name = []\n",
    "for i in smart_name:\n",
    "    Smart_Name.append(i.text)\n",
    "\n",
    "# Finding element for battery capacity using xpath\n",
    "battery = driver.find_elements_by_xpath(\"//ul[@class='_1xgFaf']//li[4]\")\n",
    "\n",
    "# We will run a loop to iterate all the battery capacity tags extracted above and extract the text inside them\n",
    "Battery = []\n",
    "for i in battery:\n",
    "    Battery.append(i.text)\n",
    "    \n",
    "# Finding element for price using xpath\n",
    "price = driver.find_elements_by_xpath(\"//div[@class='_30jeq3 _1_WHN1']\")\n",
    "\n",
    "# We will run a loop to iterate all the price tags extracted above and extract the text inside them\n",
    "Price = []\n",
    "for i in price:\n",
    "    Price.append(i.text)\n",
    "\n",
    "# Finding element for product url using xpath\n",
    "product_url = driver.find_elements_by_xpath(\"//a[@class='_1fQZEK']\")\n",
    "\n",
    "# We will run a loop to iterate all the product url tags extracted above and extract the text inside them\n",
    "Product_Url = []\n",
    "for i in product_url:\n",
    "    Product_Url.append(i.get_attribute(\"href\"))\n",
    "\n",
    "Brand_Name = []\n",
    "Color = []\n",
    "RAM = []\n",
    "ROM = []\n",
    "Pri_Cam = []\n",
    "Sec_Cam = []\n",
    "Dis_Size = []\n",
    "Dis_Res = []\n",
    "Processor = []\n",
    "Process_Cores = []\n",
    "\n",
    "for i in Product_Url:\n",
    "    driver.get(i)\n",
    "    driver.implicitly_wait(4)\n",
    "    # Clicking the read more button so that the data can be fetched using xpath\n",
    "    driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _1FH0tX\"]').click()\n",
    "    # Extracting the brand name using xpath\n",
    "    try:\n",
    "        brand_name = driver.find_element_by_xpath(\"//table[@class='_14cfVK']//tbody//tr[3]//td[2]\")\n",
    "        Brand_Name.append(brand_name.text)\n",
    "    except:\n",
    "        Brand_Name.append(\"-\")\n",
    "        \n",
    "    # Extracting the color using xpath\n",
    "    try:\n",
    "        color = driver.find_element_by_xpath(\"//table[@class='_14cfVK']//tbody//tr[4]//td[2]\")\n",
    "        Color.append(color.text)\n",
    "    except:\n",
    "        Color.append(\"-\")\n",
    "\n",
    "    # Extracting the ram using xpath\n",
    "    try:\n",
    "        ram = driver.find_element_by_xpath(\"//div[@class='_1UhVsV']/div[4]/table//tr[2]/td[2]\")\n",
    "        RAM.append(ram.text)\n",
    "    except:\n",
    "        RAM.append(\"-\")\n",
    "\n",
    "    # Extracting the rom using xpath\n",
    "    try:\n",
    "        rom = driver.find_element_by_xpath(\"//div[@class='_1UhVsV']//div[4]//table//tr[1]//td[2]\")\n",
    "        ROM.append(rom.text)\n",
    "    except:\n",
    "        ROM.append(\"-\")\n",
    "\n",
    "    # Extracting the primary camera using xpath\n",
    "    try:\n",
    "        pri_cam = driver.find_element_by_xpath(\"//div[@class='_1UhVsV']//div[5]//table//tr[2]//td[2]\")\n",
    "        Pri_Cam.append(pri_cam.text)\n",
    "    except:\n",
    "        Pri_Cam.append(\"-\")\n",
    "\n",
    "    # Extracting the secondary camera using xpath\n",
    "    try:\n",
    "        sec_cam = driver.find_element_by_xpath(\"//div[@class='_1UhVsV']//div[5]//table//tr[5]//td[2]\")\n",
    "        Sec_Cam.append(sec_cam.text)\n",
    "    except:\n",
    "        Sec_Cam.append(\"-\")\n",
    "        \n",
    "    # Extracting the display size using xpath\n",
    "    try:\n",
    "        dis_size = driver.find_element_by_xpath(\"//div[@class='_1UhVsV']//div[2]//tr[1]//td[2]\")\n",
    "        Dis_Size.append(dis_size.text)\n",
    "    except:\n",
    "        Dis_Size.append(\"-\")\n",
    "        \n",
    "    # Extracting the display resolution using xpath\n",
    "    try:\n",
    "        dis_res = driver.find_element_by_xpath(\"//div[@class='_1UhVsV']//div[2]//table//tr[2]//td[2]\")\n",
    "        Dis_Res.append(dis_res.text)\n",
    "    except:\n",
    "        Dis_Res.append(\"-\")\n",
    "\n",
    "    # Extracting the processor using xpath\n",
    "    try:\n",
    "        process = driver.find_element_by_xpath(\"//div[@class='_1UhVsV']//div[3]//table//tr[2]//td[2]\")\n",
    "        Processor.append(process.text)\n",
    "    except:\n",
    "        Processor.append(\"-\")\n",
    "        \n",
    "    # Extracting the processor cores using xpath\n",
    "    try:\n",
    "        process_cor = driver.find_element_by_xpath(\"//div[@class='_1UhVsV']//div[3]//table//tr[3]//td[2]\")\n",
    "        Process_Cores.append(process_cor.text)\n",
    "    except:\n",
    "        Process_Cores.append(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "ques_4 = pd.DataFrame({})\n",
    "ques_4['BRAND NAME'] = Brand_Name\n",
    "ques_4['SMARTPHONE NAME'] = Smart_Name\n",
    "ques_4['COLOUR'] = Color\n",
    "ques_4['RAM'] = RAM\n",
    "ques_4['ROM'] = ROM\n",
    "ques_4['PRIMARY CAMERA'] = Pri_Cam\n",
    "ques_4['SECONDARY CAMERA'] = Sec_Cam\n",
    "ques_4['DISPLAY SIZE'] = Dis_Size\n",
    "ques_4['DISPLAY RESOLUTION'] = Dis_Res\n",
    "ques_4['PROCESSOR'] = Processor\n",
    "ques_4['PROCESSOR CORES'] = Process_Cores\n",
    "ques_4['BATTERY'] = Battery\n",
    "ques_4['PRICE'] = Price\n",
    "ques_4['PRODUCT URL'] = Product_Url\n",
    "ques_4.index+=1\n",
    "\n",
    "# Details of smartphone are as follows\n",
    "ques_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the dataframe in csv file\n",
    "ques_4.to_csv(\"flipkart_smartphones.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required Libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "# importing regex\n",
    "import re\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first connect with the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\vinit\\Downloads\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_5 = \"https://www.google.com/maps/\"\n",
    "driver.get(url_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding element for search bar using xpath\n",
    "search_bar = driver.find_element_by_id(\"searchboxinput\")\n",
    "search_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing in the search bar\n",
    "search_bar.send_keys(\"Dehradun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the element for search button using xpath\n",
    "search_btn = driver.find_element_by_id(\"searchbox-searchbutton\")\n",
    "\n",
    "# Clicking the search button\n",
    "search_btn.click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    url_string = driver.current_url\n",
    "    print(\"URL Extracted: \", url_string)\n",
    "    lat_lng = re.findall(r'@(.*)data',url_string)\n",
    "    if len(lat_lng):\n",
    "        lat_lng_list = lat_lng[0].split(\",\")\n",
    "        if len(lat_lng_list)>=2:\n",
    "            lat = lat_lng_list[0]\n",
    "            lng = lat_lng_list[1]\n",
    "        print(\"Latitude = {}, Longitude = {}\".format(lat, lng))\n",
    "\n",
    "except Exception as e:\n",
    "        print(\"Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. Write a program to scrap details of all the funding deals for second quarter (i.e. July 20 –September 20) from trak.in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required Libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "# importing regex\n",
    "import re\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first connect with the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\vinit\\Downloads\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_6 = \"https://trak.in/\"\n",
    "driver.get(url_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding element for search bar for funding deals using xpath\n",
    "search_bar = driver.find_element_by_xpath(\"//a[@title='http://trak.in/india-startup-funding-investment-2015/']\")\n",
    "search_bar.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding element for date using xpath\n",
    "date = driver.find_elements_by_xpath(\"//tbody[@class='row-hover']//tr//td[2]\")\n",
    "\n",
    "# We will run a loop to iterate all the date tags extracted above and extract the text inside them\n",
    "Date = []\n",
    "for i in date:\n",
    "    Date.append(i.text)\n",
    "\n",
    "# Finding element for startup name using xpath\n",
    "start_name = driver.find_elements_by_xpath(\"//td[@class='column-3']\")\n",
    "\n",
    "# We will run a loop to iterate all the startup name tags extracted above and extract the text inside them\n",
    "Start_Name = []\n",
    "for i in start_name:\n",
    "    Start_Name.append(i.text)\n",
    "\n",
    "# Finding element for industry using xpath\n",
    "industry = driver.find_elements_by_xpath(\"//td[@class='column-4']\")\n",
    "\n",
    "# We will run a loop to iterate all the industry tags extracted above and extract the text inside them\n",
    "Industry = []\n",
    "for i in industry:\n",
    "    Industry.append(i.text)\n",
    "    \n",
    "# Finding element for sub vertical using xpath\n",
    "sub_vertical = driver.find_elements_by_xpath(\"//td[@class='column-5']\")\n",
    "\n",
    "# We will run a loop to iterate all the sub vertical tags extracted above and extract the text inside them\n",
    "Sub_Vertical = []\n",
    "for i in sub_vertical:\n",
    "    Sub_Vertical.append(i.text)\n",
    "\n",
    "# Finding element for city using xpath\n",
    "city = driver.find_elements_by_xpath(\"//td[@class='column-6']\")\n",
    "\n",
    "# We will run a loop to iterate all the city tags extracted above and extract the text inside them\n",
    "City = []\n",
    "for i in city:\n",
    "    City.append(i.text)\n",
    "\n",
    "# Finding element for investor name using xpath\n",
    "inv_name = driver.find_elements_by_xpath(\"//td[@class='column-7']\")\n",
    "\n",
    "# We will run a loop to iterate all the investor name tags extracted above and extract the text inside them\n",
    "Inv_Name = []\n",
    "for i in inv_name:\n",
    "    Inv_Name.append(i.text)\n",
    "    \n",
    "# Finding element for investment type using xpath\n",
    "inv_type = driver.find_elements_by_xpath(\"//td[@class='column-8']\")\n",
    "\n",
    "# We will run a loop to iterate all the investment type tags extracted above and extract the text inside them\n",
    "Inv_Type = []\n",
    "for i in inv_type:\n",
    "    Inv_Type.append(i.text)\n",
    "\n",
    "# Finding element for amount using xpath\n",
    "amount = driver.find_elements_by_xpath(\"//td[@class='column-9']\")\n",
    "\n",
    "# We will run a loop to iterate all the amount tags extracted above and extract the text inside them\n",
    "Amount = []\n",
    "for i in amount:\n",
    "    Amount.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "ques_6 = pd.DataFrame({})\n",
    "ques_6[\"DATE\"] = Date[55:85]\n",
    "ques_6[\"STARTUP NAME\"] = Start_Name[55:85]\n",
    "ques_6[\"INDUSTRY/VERTICAL\"] = Industry[55:85]\n",
    "ques_6[\"SUB-VERTICAL\"] = Sub_Vertical[55:85]\n",
    "ques_6[\"LOCATION\"] = City[55:85]\n",
    "ques_6[\"INVESTOR'S NAME\"] = Inv_Name[55:85]\n",
    "ques_6[\"INVESTMENT TYPE\"] = Inv_Type[55:85]\n",
    "ques_6[\"AMOUNT\"] = Amount[55:85]\n",
    "ques_6.index+=1\n",
    "\n",
    "# Details of all the funding deals\n",
    "ques_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the dataframe in csv file\n",
    "ques_6.to_csv(\"Trak_fund.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7. Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required Libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "# importing regex\n",
    "import re\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first connect with the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\vinit\\Downloads\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_7 = \"https://www.digit.in/\"\n",
    "driver.get(url_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding element for laptop using xpath\n",
    "laptop_btn = driver.find_element_by_xpath(\"//div[@class='menu']//li[3]//a\")\n",
    "laptop_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding element for best gaming laptops using xpath\n",
    "best_gaming = driver.find_element_by_xpath(\"//div[@class='Listbrand']//li[10]\")\n",
    "best_gaming.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the element for brand names using xpath\n",
    "brand = driver.find_elements_by_xpath(\"//div[@class='right-container']//div//h3\")\n",
    "\n",
    "# We will run a loop to iterate all the brand name tags extracted above and extract the text inside them\n",
    "Brand = []\n",
    "for i in brand:\n",
    "    Brand.append(i.text)\n",
    "\n",
    "# Finding the element for operating system using xpath    \n",
    "os = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[3]//td[3]\")\n",
    "\n",
    "# We will run a loop to iterate all the operating system tags extracted above and extract the text inside them\n",
    "OS = []\n",
    "for i in os:\n",
    "    OS.append(i.text)\n",
    "\n",
    "# Finding the element for display using xpath \n",
    "display = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[4]//td[3]\")\n",
    "\n",
    "# We will run a loop to iterate all the display tags extracted above and extract the text inside them\n",
    "Display = []\n",
    "for i in display:\n",
    "    Display.append(i.text)\n",
    "\n",
    "# Finding the element for processor using xpath \n",
    "pro =  driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[5]//td[3]\")\n",
    "\n",
    "# We will run a loop to iterate all the processor tags extracted above and extract the text inside them\n",
    "Pro = []\n",
    "for i in pro:\n",
    "    Pro.append(i.text)\n",
    "\n",
    "# Finding the element for memory using xpath\n",
    "memory = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[6]//td[3]\")\n",
    "\n",
    "# We will run a loop to iterate all the memory tags extracted above and extract the text inside them\n",
    "Memory = []\n",
    "for i in memory:\n",
    "    Memory.append(i.text)\n",
    "\n",
    "# Finding the element for weight using xpath\n",
    "weight = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[7]//td[3]\")\n",
    "\n",
    "# We will run a loop to iterate all the weight tags extracted above and extract the text inside them\n",
    "Weight = []\n",
    "for i in weight:\n",
    "    Weight.append(i.text)\n",
    "\n",
    "# Finding the element for dimension using xpath\n",
    "dimen =  driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[8]//td[3]\")\n",
    "\n",
    "# We will run a loop to iterate all the dimension tags extracted above and extract the text inside them\n",
    "Dimen = []\n",
    "for i in dimen:\n",
    "    Dimen.append(i.text)\n",
    "\n",
    "# Finding the element for graphic using xpath\n",
    "graph = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[9]//td[3]\")\n",
    "\n",
    "# We will run a loop to iterate all the graphic tags extracted above and extract the text inside them\n",
    "Graph = []\n",
    "for i in graph:\n",
    "    Graph.append(i.text)\n",
    "\n",
    "# Finding the element for price using xpath\n",
    "price = driver.find_elements_by_xpath(\"//table[@id='summtable']//tr//td[3]\")\n",
    "\n",
    "# We will run a loop to iterate all the price tags extracted above and extract the text inside them\n",
    "Price = []\n",
    "for i in price:\n",
    "    Price.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "ques_7 = pd.DataFrame({})\n",
    "ques_7[\"BRAND NAME\"] = Brand\n",
    "ques_7[\"OPERATING SYSTEM\"] = OS \n",
    "ques_7[\"DISPLAY\"] = Display\n",
    "ques_7[\"PROCESSOR\"] = Pro\n",
    "ques_7[\"MEMORY\"] = Memory\n",
    "ques_7[\"WEIGHT\"] = Weight\n",
    "ques_7[\"DIMENSION\"] = Dimen\n",
    "ques_7[\"GRAPHICS PROCESSOR\"] = Graph \n",
    "ques_7[\"PRICE\"] = Price\n",
    "ques_7.index+=1\n",
    "\n",
    "\n",
    "ques_7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the dataframe in csv file\n",
    "ques_7.to_csv(\"Gaming_Laptops.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required Libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "# importing regex\n",
    "import re\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first connect with the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\vinit\\Downloads\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_8 = \"https://www.forbes.com/\"\n",
    "driver.get(url_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the element for left button using xpath\n",
    "left_btn = driver.find_element_by_xpath(\"//div[@class='header__left']//button[1]\")\n",
    "left_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the element for clicking the billionaire using xpath\n",
    "billion_clk = driver.find_element_by_xpath(\"//ul[@class='header__channels']//li[1]\")\n",
    "billion_clk.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the element for clicking the all billionaires option using xpath\n",
    "all_billion = driver.find_element_by_xpath(\"//li[@class='header__section']//a\")\n",
    "all_billion.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the element for rank using xpath\n",
    "rank = driver.find_elements_by_xpath(\"//div[@class='rank']\")\n",
    "\n",
    "# We will run a loop to iterate all the rank tags extracted above and extract the text inside them\n",
    "Rank = []\n",
    "for i in rank:\n",
    "    Rank.append(i.text)\n",
    "\n",
    "# Finding the element for name using xpath\n",
    "name = driver.find_elements_by_xpath(\"//div[@class='personName']\")\n",
    "\n",
    "# We will run a loop to iterate all the name tags extracted above and extract the text inside them\n",
    "Name = []\n",
    "for i in name:\n",
    "    Name.append(i.text)\n",
    "    \n",
    "# Finding the element for net worth using xpath\n",
    "net = driver.find_elements_by_xpath(\"//div[@class='netWorth']//div\")\n",
    "\n",
    "# We will run a loop to iterate all the net worth tags extracted above and extract the text inside them\n",
    "Net = []\n",
    "for i in net:\n",
    "    Net.append(i.text)\n",
    "    \n",
    "# Finding the element for age using xpath\n",
    "age = driver.find_elements_by_xpath(\"//div[@class='age']//div\")\n",
    "\n",
    "# We will run a loop to iterate all the age tags extracted above and extract the text inside them\n",
    "Age = []\n",
    "for i in age:\n",
    "    Age.append(i.text)\n",
    "\n",
    "# Finding the element for citizenship using xpath\n",
    "city = driver.find_elements_by_xpath(\"//div[@class='countryOfCitizenship']\")\n",
    "\n",
    "# We will run a loop to iterate all the citizenship tags extracted above and extract the text inside them\n",
    "City = []\n",
    "for i in city:\n",
    "    City.append(i.text)\n",
    "\n",
    "# Finding the element for source using xpath\n",
    "source = driver.find_elements_by_xpath(\"//div[@class='source-column']\")\n",
    "\n",
    "# We will run a loop to iterate all the source tags extracted above and extract the text inside them\n",
    "Source = []\n",
    "for i in source:\n",
    "    Source.append(i.text)\n",
    "    \n",
    "# Finding the element for industry using xpath\n",
    "indus = driver.find_elements_by_xpath(\"//div[@class='category']\")\n",
    "\n",
    "# We will run a loop to iterate all the industry tags extracted above and extract the text inside them\n",
    "Indus = []\n",
    "for i in indus:\n",
    "    Indus.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "ques_8 = pd.DataFrame({})\n",
    "ques_8[\"RANK\"] = Rank\n",
    "ques_8[\"NAME\"] = Name\n",
    "ques_8[\"NET WORTH\"] = Net[::2]\n",
    "ques_8[\"AGE\"] = Age\n",
    "ques_8[\"COUNTRY/CITIZENSHIP\"] = City \n",
    "ques_8[\"SOURCE\"] = Source\n",
    "ques_8[\"INDUSTRY\"] = Indus\n",
    "ques_8.index+=1\n",
    "\n",
    "# Details for all Billionaires\n",
    "ques_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the dataframe in csv file\n",
    "ques_8.to_csv(\"billionaire_details.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required Libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "# importing regex\n",
    "import re\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first connect with the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\vinit\\Downloads\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_9 = \"https://www.youtube.com/\"\n",
    "driver.get(url_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the element for search using xpath\n",
    "search = driver.find_element_by_xpath(\"//input[@id='search']\")\n",
    "search.send_keys(\"Tech Burner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the element for search button using id\n",
    "search_btn = driver.find_element_by_id(\"search-icon-legacy\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the element for clicking the video using xpath\n",
    "clk_video = driver.find_element_by_xpath(\"//div[@class='text-wrapper style-scope ytd-video-renderer']\")\n",
    "clk_video.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the element for comments using id\n",
    "Comments = []\n",
    "for i in range(1,2):\n",
    "    try:\n",
    "        comments = driver.find_elements_by_id(\"content-text\")\n",
    "         # We will run a loop to iterate all the comment tags extracted above and extract the text inside them\n",
    "        for i in comments:\n",
    "            Comments.append(i.text)\n",
    "    except:\n",
    "        Comments.append(\"No Comment\")\n",
    "        \n",
    "len(Comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the element for time using xpath\n",
    "Time = []\n",
    "for i in range(1,2):\n",
    "    try:\n",
    "        time = driver.find_elements_by_xpath(\"//yt-formatted-string[@class='published-time-text above-comment style-scope ytd-comment-renderer']//a\")\n",
    "         # We will run a loop to iterate all the time tags extracted above and extract the text inside them\n",
    "        for i in time:\n",
    "            Time.append(i.text)\n",
    "    except:\n",
    "        Time.append(\"Error\")\n",
    "        \n",
    "len(Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the element for upvote using id\n",
    "Upvote = []\n",
    "for i in range(1,2):\n",
    "    try:\n",
    "        upvote = driver.find_elements_by_id(\"vote-count-middle\")\n",
    "         # We will run a loop to iterate all the upvote tags extracted above and extract the text inside them\n",
    "        for i in upvote:\n",
    "            Upvote.append(i.text)\n",
    "    except:\n",
    "        Upvote.append(\"Null\")\n",
    "        \n",
    "len(Upvote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the element for names using id\n",
    "Names = []\n",
    "for i in range(1,2):\n",
    "    try:\n",
    "        names = driver.find_elements_by_id(\"author-text\")\n",
    "         # We will run a loop to iterate all the name tags extracted above and extract the text inside them\n",
    "        for i in names:\n",
    "            Names.append(i.text)\n",
    "    except:\n",
    "        Names.append(\"Null\")\n",
    "        \n",
    "len(Names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "ques_9 = pd.DataFrame({})\n",
    "ques_9[\"COMMENT SECTTION\"] = Comments[:500]\n",
    "ques_9[\"NAME\"] = Names[:500]\n",
    "ques_9[\"TIME\"] = Time[:500]\n",
    "ques_9[\"UPVOTE / NUMBER OF LIKES\"] = Upvote[:500]\n",
    "ques_9.index+=1\n",
    "\n",
    "# First 500 comments on YouTube\n",
    "ques_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the dataframe in csv file\n",
    "ques_9.to_csv(\"comments_yt.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q10. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required Libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "# importing regex\n",
    "import re\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first connect with the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\vinit\\Downloads\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_10 = \"https://www.hostelworld.com/\"\n",
    "driver.get(url_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the element for search using xpath\n",
    "search = driver.find_element_by_xpath(\"//input[@id='search-input-field']\")\n",
    "search.send_keys(\"London\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slecting the second option for location using xpath\n",
    "clk_opt = driver.find_element_by_xpath(\"//li[@tabindex='-1'][2]//div\")\n",
    "clk_opt.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the element for search button using id\n",
    "search_btn = driver.find_element_by_id(\"search-button\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the element for hostel name using xpath\n",
    "hostel = driver.find_elements_by_xpath(\"//h2[@class='title title-6']\")\n",
    "\n",
    "# We will run a loop to iterate all the hostel name tags extracted above and extract the text inside them\n",
    "Hostel = []\n",
    "for i in hostel:\n",
    "    Hostel.append(i.text)\n",
    "\n",
    "# Finding the element for distance using xpath\n",
    "dist = driver.find_elements_by_xpath(\"//a[@role='button']//span[1]\")\n",
    "\n",
    "# We will run a loop to iterate all the distance tags extracted above and extract the text inside them\n",
    "Dist = []\n",
    "for i in dist:\n",
    "    Dist.append(i.text)\n",
    "\n",
    "# Finding the element for ratings using xpath\n",
    "ratings = driver.find_elements_by_xpath(\"//div[@class='score orange big']\")\n",
    "\n",
    "# We will run a loop to iterate all the rating tags extracted above and extract the text inside them\n",
    "Ratings = []\n",
    "for i in ratings:\n",
    "    Ratings.append(i.text)\n",
    "\n",
    "# Finding the element for total reviews using xpath\n",
    "total = driver.find_elements_by_xpath(\"//div[@class='reviews']\")\n",
    "\n",
    "# We will run a loop to iterate all the total review tags extracted above and extract the text inside them\n",
    "Total = []\n",
    "for i in total:\n",
    "    Total.append(i.text)\n",
    "\n",
    "# Finding the element for overall review using xpath\n",
    "comments = driver.find_elements_by_xpath(\"//div[@class='keyword']\")\n",
    "\n",
    "# We will run a loop to iterate all the overall review tags extracted above and extract the text inside them\n",
    "Comments = []\n",
    "for i in comments:\n",
    "    Comments.append(i.text)\n",
    "\n",
    "# Finding the element for private review using xpath\n",
    "private = driver.find_elements_by_xpath(\"//a//div[@class='price-col'][1]\")\n",
    "\n",
    "# We will run a loop to iterate all the private tags extracted above and extract the text inside them\n",
    "Private = []\n",
    "for i in private:\n",
    "    Private.append(i.text.replace(\"\\n\",\" \"))\n",
    "\n",
    "# Finding the element for dorm review using xpath\n",
    "dorm = driver.find_elements_by_xpath(\"//div[@class='prices-col']//a//div[2]\")\n",
    "\n",
    "# We will run a loop to iterate all the dorm tags extracted above and extract the text inside them\n",
    "Dorm = []\n",
    "for i in dorm:\n",
    "    Dorm.append(i.text.replace(\"\\n\",\" \"))\n",
    "\n",
    "# Finding the element for facilities using xpath\n",
    "facility = driver.find_elements_by_xpath(\"//div[@class='facilities-label facilities']\")\n",
    "\n",
    "# We will run a loop to iterate all the facility tags extracted above and extract the text inside them\n",
    "Facility = []\n",
    "for i in facility:\n",
    "    Facility.append(i.text.replace(\"\\n\",\" \"))\n",
    "\n",
    "# Finding element for product url using xpath\n",
    "product_url = driver.find_elements_by_xpath(\"//h2[@class='title title-6']//a\")\n",
    "\n",
    "# We will run a loop to iterate all the product url tags extracted above and extract the text inside them\n",
    "Product_Url = []\n",
    "for i in product_url:\n",
    "    Product_Url.append(i.get_attribute(\"href\"))\n",
    "    \n",
    "Desc = []\n",
    "for i in Product_Url:\n",
    "    driver.get(i)\n",
    "    # Extracting the description using xpath\n",
    "    try:\n",
    "        desc = driver.find_element_by_xpath(\"//div[@class='description-container']//div//div[2]\")\n",
    "        Desc.append(desc.text)\n",
    "    except:\n",
    "        Desc.append(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "ques_10 = pd.DataFrame({})\n",
    "ques_10[\"HOSTEL NAME\"] = Hostel\n",
    "ques_10[\"DISTANCE\"] = Dist\n",
    "ques_10[\"RATINGS\"] = Ratings\n",
    "ques_10[\"TOTAL REVIEWS\"] = Total\n",
    "ques_10[\"OVERALL REVIEWS\"] = Comments[3:]\n",
    "ques_10[\"PRIVATES FROM PRICE\"] = Private\n",
    "ques_10[\"DORMS FROM PRICE\"] = Dorm\n",
    "ques_10[\"FACILITIES\"] = Facility\n",
    "ques_10[\"PROPERTY DESC.\"] = Desc\n",
    "ques_10.index+=1\n",
    "\n",
    "# All the details are as follows:\n",
    "ques_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the dataframe in csv file\n",
    "ques_10.to_csv(\"london_hostels.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
